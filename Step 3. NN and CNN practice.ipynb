{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network and Convolutional Neural Network Practice\n",
    "\n",
    "Self study deep learning using book \"Deep Learning\" published by O'Reilly.\n",
    "\n",
    "The basic structure of neural network is cell. The cell takes input and generate output based on the input value. Similar to $y = f(x)$. $x$ is the input value, $f(x)$ is the internal function, and $y$ is the output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary package\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Step function is not a good function for activating cells in practical. The cell output values are usually processed by **Sigmoid** or **Relu** functions to create a smooth curve output. For classification problem, **Softmax** function is commonly used. For regression problem, usually use the cell output directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# sigmoid and RelU functions are commonly used for passing value from layer to layer\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x) )\n",
    "\n",
    "# ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# softmax function, usually used for clasification problem\n",
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum( np.exp(x) )\n",
    "\n",
    "# gradient of sigmoid function\n",
    "def sigmoid_grad(x):\n",
    "    return ( 1.0 - sigmoid(x) ) * sigmoid(x)\n",
    "\n",
    "# gradient of ReLU function\n",
    "\n",
    "\n",
    "# gradient of softmax function\n",
    "# gradient of softmax is same as softmax, check the formula!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "For all machine learning problems, we need a loss function to help our model learning (adjusting weights). Both mean square error function and cross entropy error function are commonly used in neural network. \n",
    "\n",
    "The cross entropy error function can be expressed as:\n",
    "\n",
    "$E = - \\sum t_k\\log y_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error functions\n",
    "\n",
    "# mse\n",
    "def mse(y, t):\n",
    "    return 0.5 * np.sum( (y-t)**2 )\n",
    "\n",
    "# cross-entropy error\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # prevent log function error\n",
    "    return -np.sum( t * np.log(y+delta) )\n",
    "\n",
    "# batch version\n",
    "#def cross_entropy_error(y, t):\n",
    "#    if y.ndim == 1:\n",
    "#        t = t.reshape(1, t.size)\n",
    "#        y = y.reshape(1, y.size)\n",
    "#    \n",
    "#    batch_size = y.shape[0]\n",
    "#    return -np.sum(np.log( y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1: 1 layer network\n",
    "\n",
    "First we build a simple 1 layer network with 2 cells can take 2 input values and can predict 3 different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple net practice\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x,self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [-0.76498923  0.01598985 -1.56366737]\n",
      "Error: 2.088882638363264\n"
     ]
    }
   ],
   "source": [
    "# initiate simple 1 layer network\n",
    "net = simpleNet()\n",
    "\n",
    "# given inputs x1, x2 = 0.6, 0.9\n",
    "x = np.array([0.6, 0.9])\n",
    "\n",
    "# predict y\n",
    "y_hat = net.predict(x)\n",
    "print('Prediction: ' + str(y_hat))\n",
    "\n",
    "# assume actual result is [0, 0, 1]\n",
    "t =np.array([0, 0, 1])\n",
    "\n",
    "# cross entropy error\n",
    "error = net.loss(x, t)\n",
    "print('Error: ' + str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 2: 2 layers network\n",
    "\n",
    "Second, build a simple 2 layers network with 100 cells. The input and output size are dynamic. The initial weights are set using random function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layers\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        \n",
    "        # 1st layer size: from input to cell size\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        \n",
    "        # 2nd layer size: from cell size to output size\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # get weights\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        # first layer output: input * W1 + b1\n",
    "        # use sigmoid function to smooth values\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        \n",
    "        # second layer output: input * W2 + b2\n",
    "        # use softmax to normalize the result\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # numerical gradient\n",
    "    # the purpose of numerical gradient function is to show how slow it is.\n",
    "    # graph gradient method is much faster\n",
    "    def numerical_gradient(self, x, t):\n",
    "        pass\n",
    "        #loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        #grads = {}\n",
    "        #grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        #grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        #grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        #grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        #return grads\n",
    "    \n",
    "    # graph gradient\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y-t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis = 0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis = 0)\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input * 1st layer cells: (784, 100)\n",
      "Size of 1st layer bias vecotr: (100,)\n",
      "Size of input * 1st layer cells: (100, 10)\n",
      "Size of 2nd layer bias vecotr: (10,)\n"
     ]
    }
   ],
   "source": [
    "# initiate two layers network with input size 784 (equals to 28*28 image size), \n",
    "# hidden layer with 100 cells, and output size 10 as 0~9 digits.\n",
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size= 10)\n",
    "\n",
    "# show the matrix size\n",
    "print('Size of input * 1st layer cells: ' + str(net.params['W1'].shape))\n",
    "print('Size of 1st layer bias vecotr: ' + str(net.params['b1'].shape))\n",
    "print('Size of input * 1st layer cells: ' +str(net.params['W2'].shape))\n",
    "print('Size of 2nd layer bias vecotr: ' + str(net.params['b2'].shape))\n",
    "\n",
    "# random assign value of 100 pics (28*28 pixel)\n",
    "x = np.random.rand(100,784)\n",
    "\n",
    "# prediction\n",
    "y = net.predict(x)\n",
    "\n",
    "# random assign labels\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "# compute gradients\n",
    "# skip numerical gradient, too slow\n",
    "#numerical_grads = net.numerical_gradient(x, t)\n",
    "\n",
    "# computational graph gradients\n",
    "#compute_graph_grads = net.gradient(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important concept is to understand how to use computational graph to estimate the gradient. The method is way faster than numerical gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply 2 layers model to train and test MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mnist import MNIST\n",
    "mnidata = MNIST(os.getcwd()+'/Data')\n",
    "train_img, train_lab = mnidata.load_training()\n",
    "test_img, test_lab = mnidata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_img)\n",
    "train_label = np.array(train_lab)\n",
    "test_data = np.array(test_img)\n",
    "test_label = np.array(test_lab)\n",
    "\n",
    "# convert label to one hot encoding\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "train_label = get_one_hot(train_label, 10)\n",
    "test_label = get_one_hot(test_label, 10)\n",
    "\n",
    "# normalize\n",
    "#train_data[train_data>0] = 1\n",
    "#test_data[test_data>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mnist data\n",
    "# use 50 cells only\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 60000\n",
      "Testing data size: 10000\n",
      "1 epoch needs 600.0iteration\n",
      "Note: epoch equals to cover all training data\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "iteration_number = 1200\n",
    "train_data_size = train_data.shape[0]\n",
    "batch_data_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_lost_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_data_size/batch_data_size, 1)\n",
    "\n",
    "print('Training data size: ' + str(train_data.shape[0]))\n",
    "print('Testing data size: ' + str(test_data.shape[0]))\n",
    "print('1 epoch needs '+ str(iter_per_epoch) + 'iteration' )\n",
    "print('Note: epoch equals to cover all training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for i in range(iteration_number):\n",
    "    # batch\n",
    "    batch_mask = np.random.choice(train_data_size, batch_data_size)\n",
    "    traing_data_batch = train_data[batch_mask]\n",
    "    train_label_batch = train_label[batch_mask]\n",
    "    \n",
    "    # compute gradient\n",
    "    grad = network.gradient(traing_data_batch, train_label_batch)\n",
    "    \n",
    "    # update weights\n",
    "    # W' = W - learning_rate * dW\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # record loss\n",
    "    loss = network.loss(traing_data_batch, train_label_batch)\n",
    "    train_lost_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(train_data, train_label)\n",
    "        test_acc = network.accuracy(test_data, test_label)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
