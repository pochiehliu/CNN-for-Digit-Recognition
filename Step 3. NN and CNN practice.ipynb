{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network and Convolutional Neural Network Practice\n",
    "\n",
    "Self learning practicing code. Textbook: Deep learning O'Reilly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Because step function is not a good function for activation cell in practical use. Sigmoid and relu functions are commonly used as replacement methods. Softmax function is used for multi-class clasification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# sigmoid function, usually used for 2 classes clasification problem\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x) )\n",
    "\n",
    "# gradient of sigmoid function\n",
    "def sigmoid_grad(x):\n",
    "    return ( 1.0 - sigmoid(x) ) * sigmoid(x)\n",
    "\n",
    "# ReLu\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# soft max function, usually used for multi-class clasification problem\n",
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum( np.exp(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "For all machine learning problems, we need a loss function to help our model learning. Both mean square error function and cross entropy error function are commonly used in neural network. \n",
    "\n",
    "$E = - \\sum t_k\\log y_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error functions\n",
    "\n",
    "# mse\n",
    "def mse(y, t):\n",
    "    return 0.5 * np.sum( (y-t)**2 )\n",
    "\n",
    "# cross-entropy error\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # prevent log function error\n",
    "    return -np.sum( t * np.log(y+delta) )\n",
    "\n",
    "# batch version\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log( y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1: 1 layer network\n",
    "Build a simple 1 layer network with 2 cells and 3 different classes to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple net practice\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x,self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "[ 2.01593832 -0.17561157  1.47297297]\n",
      "Error: \n",
      "-1.069329044419211\n"
     ]
    }
   ],
   "source": [
    "# 1 layer 3 cells simple network\n",
    "net = simpleNet()\n",
    "\n",
    "# assume inputs x1, x2 = 0.6, 0.9\n",
    "x = np.array([0.6, 0.9])\n",
    "y_hat = net.predict(x)\n",
    "print('Prediction: ')\n",
    "print(y_hat)\n",
    "\n",
    "# assume actual result = 0, 0, 1\n",
    "t =np.array([0, 0, 1])\n",
    "\n",
    "# cross entropy error\n",
    "error = net.loss(x, t)\n",
    "print('Error: ')\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 2: 2 layers network\n",
    "Build a simple 2 layers network with 100 cells in each layer to predict 10 different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layers\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        \n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        \n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # numerical gradient\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # graph gradient\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y-t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis = 0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis = 0)\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the matrix size is important. From input (784) to 1st layer, the computational equations can be expressed as matrix form x(784) * W1 (784,100) and bias term b1 (100,1). From later1 output to layer2 output, the equations can be expressed as (x * W1 + b1) * W2 (100,10) + b2 (10,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# initiate two layers with 100 cells and output to predict 10 digits\n",
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size= 10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)\n",
    "\n",
    "# assume 100 pics (28*28 pixel)\n",
    "x = np.random.rand(100,784)\n",
    "y = net.predict(x)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "# gradients\n",
    "numerical_grads = net.numerical_gradient(x, t)\n",
    "compute_graph_grads = net.gradient(x, t)\n",
    "print('Numerical gradients: ')\n",
    "print(numerical_grads)\n",
    "print('Computational graph gradients: ')\n",
    "print(compute_graph_grads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above practice is to know that computing numerical gradient is very slow. Using computational graph method is way faster than numerical gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply 2 layers model to train and test MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnidata = MNIST(os.getcwd()+'/Data')\n",
    "train_img, train_lab = mnidata.load_training()\n",
    "test_img, test_lab = mnidata.load_testing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
