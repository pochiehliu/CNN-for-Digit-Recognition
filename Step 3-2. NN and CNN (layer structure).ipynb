{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network and Convolutional Neural Network Practice\n",
    "\n",
    "In step 3-2, we want to implement computational graph concept into our NN and CNN program. Thus we have to rewrite all operations into layers. It's easier to compute the forward and backward propagation in this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T # convert x_i from row to column\n",
    "        x = x - np.max(x, axis = 0) # axis = 0: vertical \n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
    "        return y.T # transpose back to original format\n",
    "\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "# for one-hot-encoding label\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y+delta)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add and Multiply Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df/dx = df/dz * dz/dx\n",
    "# z = x + y, dz/dx = 1\n",
    "class AddLayer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    def backward(self, dfdz):\n",
    "        return dfdz*1, dfdz*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = x * y, df/dx = df/dz * dz/dx, dz/dx = y\n",
    "class MulLayer():\n",
    "    def __init__(self):\n",
    "        self.x = self.y = None\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "    def backward(self, dfdz):\n",
    "        return dfdz * self.y, dfdz * self.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test our classes\n",
    "\n",
    "Test a simple case $f(x_i,y_i,r) = (x_1*y_1+x_2*y_2)*r$, then check $df/dr = (x_1*y_1+x_2*y_2)$ and $df/d(x_1y_1+x_2y_2) = r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward check: \n",
      "( x1*y1 + x2*y2 ) * r =715.0000000000001\n",
      "\n",
      "Backward check: \n",
      "x1*y1 + x2*y2 = 650\n",
      "r = 1.1\n",
      "df/dr =650\n",
      "df/dxy = 1.1\n"
     ]
    }
   ],
   "source": [
    "# initiate values\n",
    "x1 = 100\n",
    "y1 = 2\n",
    "x2 = 150\n",
    "y2 = 3\n",
    "r = 1.1\n",
    "\n",
    "# compute forward, ie f = ( x1*y1 + x2*y2 ) * r\n",
    "# initiate layers according to corresponding computation graph\n",
    "x1y1_layer = MulLayer()\n",
    "x2y2_layer = MulLayer()\n",
    "x1y1_x2y2_layer = AddLayer()\n",
    "xyr_layer = MulLayer()\n",
    "\n",
    "# computate forward\n",
    "x1y1 = x1y1_layer.forward(x1,y1)\n",
    "x2y2 = x2y2_layer.forward(x2,y2)\n",
    "x1y1_x2y2 = x1y1_x2y2_layer.forward(x1y1,x2y2)\n",
    "xyr = xyr_layer.forward(x1y1_x2y2, r)\n",
    "print('Forward check: ')\n",
    "print('( x1*y1 + x2*y2 ) * r =' + str(xyr))\n",
    "print('')\n",
    "\n",
    "# compute backward\n",
    "df = 1\n",
    "dall, dfdr = xyr_layer.backward(df)\n",
    "dx1y1, dx2y2 = x1y1_x2y2_layer.backward(dall)\n",
    "dx1, dy1 = x1y1_layer.backward(dx1y1)\n",
    "dx2, dy2 = x2y2_layer.backward(dx2y2)\n",
    "print('Backward check: ')\n",
    "print('x1*y1 + x2*y2 = ' + str(x1y1_x2y2))\n",
    "print('r = ' + str(r))\n",
    "print('df/dr =' + str(dfdr))\n",
    "print('df/dxy = ' + str(dall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu and Sigmoid Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = x if x > 0, 0 if x <= 0, df/dx = df/dz * dz/dx, dz/dx = 1 if x > 0, 0 if x <= 0\n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    def backward(self, dfdz):\n",
    "        out = dfdz.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = 1 / (1 + exp(-x) ), \n",
    "# dz/dx = z^2*exp(-x) = 1/(1+exp(-x))^2 * exp(-x) = 1/(1+exp(-x)) * exp(-x)/(1+exp(-x)) = z * (1-z)\n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "    def backward(self, dfdz):\n",
    "        return dfdz * (1- self.out) * self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine Layer\n",
    "\n",
    "Previous step, we processed input value using matrix operation: $ Y = X\\cdot W + B $. We can rewrite there operations as a layer like we just did. This layer usually called Affine Layer in Neural Netwrok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine():\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.W) + self.b\n",
    "    \n",
    "    def backwrd(self,dfdy):\n",
    "        dx = np.dot(dfdy, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dfdy)\n",
    "        self.db = np.sum(dfdy, axis = 0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithError():\n",
    "    def __init__(self):\n",
    "        self.error = None\n",
    "        self.y_h = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.y = y\n",
    "        self.y_h = softmax(x)\n",
    "        self.error = cross_entropy_error(self.y_h, self.y)\n",
    "        \n",
    "        return self.error\n",
    "    \n",
    "    def backward(self):\n",
    "        batch_data_size = self.y.shape[0]\n",
    "        dx = (self.y_h - self.y) / batch_data_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild Two Layer Network Using Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        \n",
    "        # 1st layer size: from input to cell size\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        \n",
    "        # 2nd layer size: from cell size to output size\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # generate layers\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine_layer_1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu_layer_1' ] = Relu()\n",
    "        self.layers['Affine_layer_1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.last_layer = SoftmaxWithError()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def error(self, x, y):\n",
    "        y_h = self.predict(x)\n",
    "        return self.last_layer.forward(y_h, y)\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        y_h = self.predict(x)\n",
    "        y_h = np.argmax(y_h, axis = 1)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        return np.sum(y_h == y)/float(x.shape[0])\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        # forward\n",
    "        self.error(x, y)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
